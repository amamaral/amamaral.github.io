{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee1320d4-57a0-4f29-a822-fbe4b351fd87",
   "metadata": {},
   "source": [
    "# Multiprocess performance of linear algebra and FFTs in Python\n",
    "\n",
    "Numpy may use several backends for improving the calculation speed. Which one is currently active? Also, let's see how faster can we improve the numpy calculations using more optimized routines. In particular, it should be noticed that after installing Intel's MKL optimized numpy there was a significant improvement in the speed. While the number of cores used in the computation must be set before loading numpy (as in the examples below), the examples below do not use all cores at once.\n",
    "\n",
    "## Basic implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6246d46d-53d1-4344-adde-4a42744d452c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blas_mkl_info:\n",
      "    libraries = ['mkl_rt']\n",
      "    library_dirs = ['C:/Users/Anderson/anaconda3\\\\Library\\\\lib']\n",
      "    define_macros = [('SCIPY_MKL_H', None), ('HAVE_CBLAS', None)]\n",
      "    include_dirs = ['C:/Users/Anderson/anaconda3\\\\Library\\\\include']\n",
      "blas_opt_info:\n",
      "    libraries = ['mkl_rt']\n",
      "    library_dirs = ['C:/Users/Anderson/anaconda3\\\\Library\\\\lib']\n",
      "    define_macros = [('SCIPY_MKL_H', None), ('HAVE_CBLAS', None)]\n",
      "    include_dirs = ['C:/Users/Anderson/anaconda3\\\\Library\\\\include']\n",
      "lapack_mkl_info:\n",
      "    libraries = ['mkl_rt']\n",
      "    library_dirs = ['C:/Users/Anderson/anaconda3\\\\Library\\\\lib']\n",
      "    define_macros = [('SCIPY_MKL_H', None), ('HAVE_CBLAS', None)]\n",
      "    include_dirs = ['C:/Users/Anderson/anaconda3\\\\Library\\\\include']\n",
      "lapack_opt_info:\n",
      "    libraries = ['mkl_rt']\n",
      "    library_dirs = ['C:/Users/Anderson/anaconda3\\\\Library\\\\lib']\n",
      "    define_macros = [('SCIPY_MKL_H', None), ('HAVE_CBLAS', None)]\n",
      "    include_dirs = ['C:/Users/Anderson/anaconda3\\\\Library\\\\include']\n"
     ]
    }
   ],
   "source": [
    "# See http://mitrocketscience.blogspot.com/2018/11/automatic-mulit-threading-with-python.html\n",
    "\n",
    "# Also notice that I've installed Intel's mkl as suggested at\n",
    "# https://www.intel.com/content/www/us/en/developer/articles/technical/using-intel-distribution-for-python-with-anaconda.html\n",
    "\n",
    "import os\n",
    "\n",
    "NTHREADS = '12' # Value set as string\n",
    "\n",
    "# Export system variables before loading numpy\n",
    "for i in ['OMP_NUM_THREADS', 'OPENBLAS_NUM_THREADS', 'MKL_NUM_THREADS']:#,\n",
    "          #'VECLIB_MAXIMUM_THREADS', 'NUMEXPR_NUM_THREADS']:\n",
    "    os.environ[i] = NTHREADS\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Defining two big matrices for testing\n",
    "\n",
    "matrix_shape = (2048, 2048)\n",
    "a = np.random.random(matrix_shape) + 1j * np.random.random(matrix_shape)\n",
    "b = np.random.random(matrix_shape) + 1j * np.random.random(matrix_shape)\n",
    "\n",
    "np.show_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "721f02c1-e532-48bc-9564-d35fa0b020a3",
   "metadata": {},
   "source": [
    "### Basic numpy (after adding Intel's MKL) using single core\n",
    "\n",
    "While I don't have the record for the benchmarks below before adding [Intel's MKL](https://www.intel.com/content/www/us/en/developer/articles/technical/using-intel-distribution-for-python-with-anaconda.html), some of the operations below were already significantly improved at single-core level with Intel's optimizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "653fbf09-3908-4d9b-9846-6e08ca16a962",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.27 s ± 4.66 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "# How long does it take to calculate the dot product?\n",
    "%timeit np.dot(a, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5180504-d79a-42f1-bb85-77307ac9121b",
   "metadata": {},
   "source": [
    "Looking at Windows' resources monitor, it can be clearly seen that only a single core is working at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a36a12ac-8692-4119-b875-7be602cae42f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37.1 ms ± 449 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit np.fft.fft(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "858ed028-a2b9-4c53-91de-2a30d9cd02cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38.9 ms ± 590 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit np.fft.ifft(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7b941b52-4566-47a0-bec0-96869ae7c32c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17.3 ms ± 1.18 ms per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -n 100 a * b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca16688-9226-45e9-b3bf-623c1f6ef7f3",
   "metadata": {},
   "source": [
    "### pyfftw\n",
    "\n",
    "pyfftw is an interface to FFTW, which claims to be a very fast FFT library. Below we can see how fast does it perform without optimization. It should be noticed that pyfftw states that significant improvements can be seen by tuning the library calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "82777e97-34b4-4499-84a8-dd84044ae487",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyfftw\n",
    "\n",
    "pyfftw.config.NUM_THREADS = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "efa0a54c-d684-41e6-bc49-2e6e5ec16cf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63.3 ms ± 1.4 ms per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -n 100 pyfftw.interfaces.numpy_fft.fft(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6dff7ae-3a90-4c38-b5c7-d092c2cb1f29",
   "metadata": {},
   "source": [
    "so it seems that Intel's optimized numpy is faster than pyfftw.\n",
    "\n",
    "### Numpy and pyfft\n",
    "\n",
    "Now let's consider increasing the MKL and pyfftw number of threads to 12 (the max in my current cpu) and running again the tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "05aa75bd-c4cb-4be1-a6a7-46bfc0702bb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "322 ms ± 8.4 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit np.dot(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8cb52c8f-1f1b-47ed-a9c3-39823acf6e5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'An improvement of 74.65%'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f\"An improvement of {(1-322/1270) * 100:.2f}%\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "25b33d3a-3590-414d-9596-7e247a48ff6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33.4 ms ± 620 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit np.fft.fft(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3a45bf11-238a-48ce-9aa2-49328d302097",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'An improvement of 9.97%'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f\"An improvement of {(1-33.4/37.1) * 100:.2f}%\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "261cbb29-3aab-4e5f-a741-b1758c19c9d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35.1 ms ± 337 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit np.fft.ifft(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "87e8e68f-a1eb-4a6c-bbbd-64f55bd38b13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'An improvement of 9.77%'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f\"An improvement of {(1-35.1/38.9) * 100:.2f}%\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "32297339-0614-40d0-863d-3a0f8a0f10e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.2 ms ± 242 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -n 100 a * b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e6c79900-a915-478c-a19e-1c18c2bc4c84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'An improvement of 23.70%'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f\"An improvement of {(1-13.2/17.3) * 100:.2f}%\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7e1b97fe-7b7e-4e1b-a4e6-b6590b0faf7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46.4 ms ± 259 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "pyfftw.config.NUM_THREADS = pyfftw.config.multiprocessing.cpu_count()\n",
    "\n",
    "%timeit -n 100 pyfftw.interfaces.numpy_fft.fft(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a7cb06bd-f67d-4ea0-8f62-ed67c37b9964",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'An improvement of 26.70%'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f\"An improvement of {(1-46.4/63.3) * 100:.2f}%\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eebd1797-0958-4bd6-91e7-4c8ae78a606c",
   "metadata": {},
   "source": [
    "Some operations as the dot product have become significantly faster, but the speed increase in other expressions was not so significant. Perhaps the array size is too small for significant improvements and the process creation/destruction overhead dominates in this case. pyfftw became faster, but the numpy's fft remains faster."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "481ec0fe-985e-4912-b862-e17647778a05",
   "metadata": {},
   "source": [
    "## Numba\n",
    "\n",
    "Numba is a python library that often speeds up numerical calculations by using just-int-time compilation and avoiding the overhead of dynamic types in python scripts. Here we install it and perform some benchmarks. Unfortunately it does not support ffts, which would be a significant improvement for algorithms as split-step propagation method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ad46a578-fd2a-4e4a-a9c0-d5b07e99989b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numba in c:\\users\\anderson\\anaconda3\\lib\\site-packages (0.54.1)\n",
      "Requirement already satisfied: llvmlite<0.38,>=0.37.0rc1 in c:\\users\\anderson\\anaconda3\\lib\\site-packages (from numba) (0.37.0)\n",
      "Requirement already satisfied: numpy<1.21,>=1.17 in c:\\users\\anderson\\anaconda3\\lib\\site-packages (from numba) (1.20.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\anderson\\anaconda3\\lib\\site-packages (from numba) (58.0.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install numba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "523536c0-d19f-4368-a2af-3700f62dbeb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import jit, njit, objmode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "56e50cb6-4757-4c64-98bb-f5b217092319",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "586 ms ± 8.3 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit np.dot(a, b) + np.exp(a * b **2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1e82fbc9-117e-43b5-8bbf-e4a98b4ed468",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "396 ms ± 11.4 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "@njit\n",
    "def example():\n",
    "    return np.dot(a, b) + np.exp(a * b **2)\n",
    "\n",
    "%timeit example()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "5b7f31d9-4ef2-422d-beeb-db79c592f028",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'An improvement of 32.42%'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f\"An improvement of {(1-396/586) * 100:.2f}%\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0e36aa6f-8a07-4223-8ff9-0255a4892a30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "336 ms ± 3.62 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "def example():\n",
    "    np.fft.fft2(a * np.conj(b))*np.conj(b)\n",
    "    np.fft.ifft2(a * b)*b\n",
    "    return 0\n",
    "\n",
    "%timeit example()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "95608176-face-49f7-b405-0cb43dba050b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "277 ms ± 8.44 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Since numba does not have native support for fft,\n",
    "there is a workaround suggested at\n",
    "https://github.com/numba/numba/issues/5864#issuecomment-690838747\n",
    "\"\"\"\n",
    "\n",
    "@jit(nopython=True)\n",
    "def example2():\n",
    "    d = a * np.conj(b)\n",
    "    with objmode(d='complex128[:]'):\n",
    "        d = np.fft.fft2(d)        \n",
    "    d = d * np.conj(b)\n",
    "    \n",
    "    d = a * b\n",
    "    with objmode(d='complex128[:]'):\n",
    "        d = np.fft.ifft2(d)        \n",
    "    d = d * b    \n",
    "    return 0\n",
    "\n",
    "%timeit example2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d3f0cc6d-3970-44ca-afc1-abd3238b49c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'An improvement of 17.56%'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f\"An improvement of {(1-277/336) * 100:.2f}%\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "fe6125b3-540c-4b0a-94f0-e83a1a32da02",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Anderson\\AppData\\Local\\Temp/ipykernel_22088/3408064900.py:1: NumbaWarning: \u001b[1m\n",
      "Compilation is falling back to object mode WITH looplifting enabled because Function \"example3\" failed type inference due to: \u001b[1m\u001b[1mUnknown attribute 'fft2' of type Module(<module 'numpy.fft' from 'C:\\\\Users\\\\Anderson\\\\anaconda3\\\\lib\\\\site-packages\\\\numpy\\\\fft\\\\__init__.py'>)\n",
      "\u001b[1m\n",
      "File \"..\\..\\..\\..\\AppData\\Local\\Temp\\ipykernel_22088\\3408064900.py\", line 3:\u001b[0m\n",
      "\u001b[1m<source missing, REPL/exec in use?>\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[0m\u001b[1mDuring: typing of get attribute at C:\\Users\\Anderson\\AppData\\Local\\Temp/ipykernel_22088/3408064900.py (3)\u001b[0m\n",
      "\u001b[1m\n",
      "File \"..\\..\\..\\..\\AppData\\Local\\Temp\\ipykernel_22088\\3408064900.py\", line 3:\u001b[0m\n",
      "\u001b[1m<source missing, REPL/exec in use?>\u001b[0m\n",
      "\u001b[0m\n",
      "  @jit\n",
      "C:\\Users\\Anderson\\anaconda3\\lib\\site-packages\\numba\\core\\object_mode_passes.py:151: NumbaWarning: \u001b[1mFunction \"example3\" was compiled in object mode without forceobj=True.\n",
      "\u001b[1m\n",
      "File \"..\\..\\..\\..\\AppData\\Local\\Temp\\ipykernel_22088\\3408064900.py\", line 1:\u001b[0m\n",
      "\u001b[1m<source missing, REPL/exec in use?>\u001b[0m\n",
      "\u001b[0m\n",
      "  warnings.warn(errors.NumbaWarning(warn_msg,\n",
      "C:\\Users\\Anderson\\anaconda3\\lib\\site-packages\\numba\\core\\object_mode_passes.py:161: NumbaDeprecationWarning: \u001b[1m\n",
      "Fall-back from the nopython compilation path to the object mode compilation path has been detected, this is deprecated behaviour.\n",
      "\n",
      "For more information visit https://numba.pydata.org/numba-doc/latest/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit\n",
      "\u001b[1m\n",
      "File \"..\\..\\..\\..\\AppData\\Local\\Temp\\ipykernel_22088\\3408064900.py\", line 1:\u001b[0m\n",
      "\u001b[1m<source missing, REPL/exec in use?>\u001b[0m\n",
      "\u001b[0m\n",
      "  warnings.warn(errors.NumbaDeprecationWarning(msg,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "343 ms ± 10.3 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "@jit()\n",
    "def example3():\n",
    "    np.fft.fft2(a * np.conj(b))*np.conj(b)\n",
    "    np.fft.ifft2(a * b)*b\n",
    "    return 0\n",
    "\n",
    "%timeit example3()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "de0d098f-b0b7-4a7d-bf54-f3b06623b50c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "344 ms ± 13.5 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit example3()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a0784956-ea17-4f7e-8d1e-ac32736fe341",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'An improvement of -2.38%'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f\"An improvement of {(1-344/336) * 100:.2f}%\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fd5c35e-8218-4c21-9bf1-364920669fc6",
   "metadata": {},
   "source": [
    "Thus, while numba is able to deliver improvements around 20-30%, it's lack of fft support slows down the computation significantly. In particular the `nopython=True` option is critical to have significant performance gains, otherwise it can even make the code slower as in the example 3 above.\n",
    "\n",
    "## Tensorflow\n",
    "\n",
    "Since tensorflow has native support for ffts, some tests were also performed using the code below. I've tested it using my GPUless PC and also google colab's cloud computing service. Since it's not feasible to show the timeit results inline with so many configurations, the data is summarized at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3062f33a-974b-47d1-8746-745893daf1f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import tensorflow.experimental.numpy as tnp\n",
    "tnp.experimental_enable_numpy_behavior()\n",
    "\n",
    "# Gerando matrizes equivalentes ao teste anterior, no tensorflow\n",
    "\n",
    "matrix_shape = (2048, 2048)\n",
    "\n",
    "a = tf.random.uniform(matrix_shape, dtype=tf.dtypes.float64)\n",
    "b = tf.random.uniform(matrix_shape, dtype=tf.dtypes.float64)\n",
    "\n",
    "c = tf.cast(a, tf.dtypes.complex128) + 1j * tf.cast(b, tf.dtypes.complex128)\n",
    "d = tf.cast(b**2 - a**2, tf.dtypes.complex128) + 1j * tf.cast(a*b, tf.dtypes.complex128)\n",
    "\n",
    "a = c\n",
    "b = d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c69b2e79-fa38-4ce0-8777-d4c9228df45f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shows if a gpu is available\n",
    "tf.config.list_physical_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb73cc5f-9369-44c0-b9aa-ed2f257db11e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.debugging.set_log_device_placement(True)\n",
    "\n",
    "# Example 0\n",
    "%timeit tf.experimental.numpy.dot(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c9ccb64-ec36-4e1e-9e8b-44d3c3d93ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.debugging.set_log_device_placement(True)\n",
    "\n",
    "# Example 1\n",
    "%timeit tf.signal.fft2d(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb1d476-0858-42db-a2e8-e324cbd3a18a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.debugging.set_log_device_placement(True)\n",
    "\n",
    "# Example 2\n",
    "%timeit tf.signal.ifft2d(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b297f3-73f0-4211-a457-628d250cc948",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.debugging.set_log_device_placement(True)\n",
    "\n",
    "# Example 3\n",
    "%timeit -n 100 a * b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad2a7c4b-0821-4469-a9b7-0b491c881cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.debugging.set_log_device_placement(True)\n",
    "\n",
    "# Example 4\n",
    "%timeit tf.signal.fft( a * b) * b * tf.signal.ifft( a * b) * b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "98db014d-25fe-4271-90bb-5cdfdecc65e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Home (CPU)</th>\n",
       "      <th>Colab (CPU)</th>\n",
       "      <th>Colab (GPU-fastest)</th>\n",
       "      <th>Colab (GPU-slowest)</th>\n",
       "      <th>Colab GPU % improvement</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>783.0</td>\n",
       "      <td>3460.0</td>\n",
       "      <td>1.230</td>\n",
       "      <td>17.99490</td>\n",
       "      <td>35994.985670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>140.0</td>\n",
       "      <td>477.0</td>\n",
       "      <td>0.266</td>\n",
       "      <td>402.99000</td>\n",
       "      <td>236.574285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>173.0</td>\n",
       "      <td>605.0</td>\n",
       "      <td>3.510</td>\n",
       "      <td>3.51000</td>\n",
       "      <td>17236.467236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14.9</td>\n",
       "      <td>18.1</td>\n",
       "      <td>0.093</td>\n",
       "      <td>9.11214</td>\n",
       "      <td>393.258549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>218.0</td>\n",
       "      <td>503.0</td>\n",
       "      <td>2.940</td>\n",
       "      <td>2.94000</td>\n",
       "      <td>17108.843537</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Home (CPU)  Colab (CPU)  Colab (GPU-fastest)  Colab (GPU-slowest)  \\\n",
       "0       783.0       3460.0                1.230             17.99490   \n",
       "1       140.0        477.0                0.266            402.99000   \n",
       "2       173.0        605.0                3.510              3.51000   \n",
       "3        14.9         18.1                0.093              9.11214   \n",
       "4       218.0        503.0                2.940              2.94000   \n",
       "\n",
       "   Colab GPU % improvement  \n",
       "0             35994.985670  \n",
       "1               236.574285  \n",
       "2             17236.467236  \n",
       "3               393.258549  \n",
       "4             17108.843537  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame()\n",
    "\n",
    "#times in ms\n",
    "\n",
    "df['Home (CPU)'] = [783, 140, 173, 14.9, 218]\n",
    "df['Colab (CPU)'] = [3460, 477, 605, 18.1, 503]\n",
    "df['Colab (GPU-fastest)'] = [1.23, 0.266, 3.51, 0.093, 2.94]\n",
    "df['Colab (GPU-slowest)'] = [1.23*14.63, 0.266*1515, 3.51, 0.093*97.98, 2.94]\n",
    "\n",
    "df['Colab GPU % improvement'] = df['Colab (CPU)']*2./(df['Colab (GPU-fastest)']+df['Colab (GPU-slowest)']) * 100\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c9d7aa7-9bd4-4549-afe4-dc309fa22dfc",
   "metadata": {},
   "source": [
    "It's worth to mention that the CPU available on colab performs slower than the one I have at home, but tensorflow shows a significant performance improvement when executed on the GPU. Also, when executing locally it can be seen that tensorflow code is very optimized to use all cores. While this strategy may not be optimal for a processor with a few cores, it's certainly interesting for GPUs with their massive number of parallel processing cores.\n",
    "\n",
    "Something to be aware of is that timeit on colab works differently, such that it reports the best time, while on jupyter lab it reports the mean time. When the calculation can buffer some sections of the calculus for speedup, the reported value may be an optimistic view of the true computation time. Is it time for a GPU?\n",
    "\n",
    "Also, the timeit results from colab are inconsistent. Depending on the run, the values may become significantly larger or smaller. However, it seems reasonable to expect at least one order of magnitude speedup factor when using tensorflow+gpu for these calculations. However it should be possible to speedup up to 4 orders of magnitude, depending on the operations used. This would be very consistent with the performance gains of up to 1000x reported in \n",
    "\n",
    "https://arxiv.org/abs/2010.08895\n",
    "\n",
    "https://www.technologyreview.com/2020/10/30/1011435/ai-fourier-neural-network-cracks-navier-stokes-and-partial-differential-equations/\n",
    "\n",
    "but without the need for NNs to solve these PDEs, but it's just related with the TF's improved speed when using gpus."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1fd43fa-25de-40a5-bf53-c2a6da1e1547",
   "metadata": {},
   "source": [
    "## Pytorch\n",
    "\n",
    "Notice that pytorch also has a native fft library, and it probably should be interesting to benchmark it as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2934d034-992a-4cb4-9bc0-078152e4fcb2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
